{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: SearchEngine is an alias for the SparseRetriever\n",
    "from retriv import SearchEngine\n",
    "import csv\n",
    "from typing import Dict\n",
    "import csv \n",
    "import collections\n",
    "import jieba\n",
    "csv.field_size_limit(500 * 1024 * 1024)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去掉停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set()\n",
    "with open(\"../cn_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        stopwords.add(line.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理 corpus.tsv 并构造bm25搜索引擎\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.032 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "1000000it [04:31, 3688.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词完成\n",
      "courpus_len == 1000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '1637165', 'text': '赵雷 原创 歌曲   张韶涵 唱火'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 对corpus 进行分词\n",
    "pid_to_passage = []\n",
    "datapath = \"corpus.tsv\"\n",
    "\n",
    "def create_id_item_dict(file_path: str, delimiter: str = \"\\t\") -> Dict[int, str]:  ## note:数据准\n",
    "\n",
    "    print(file_path)\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "\n",
    "        for idx, (qid, passage) in enumerate(tqdm(csv.reader(f, delimiter=delimiter))):  ## ['qid', 'pid', 'index']\n",
    "\n",
    "            pid_to_passage.append((qid, passage, [word for word in jieba.cut(passage) if word not in stopwords]))\n",
    "\n",
    "\n",
    "create_id_item_dict(datapath)\n",
    "print(\"分词完成\")\n",
    "pid_to_passage_to_save = []\n",
    "for data in pid_to_passage:\n",
    "    data_temp = {}\n",
    "    data_temp[\"id\"] = data[0]\n",
    "    data_temp[\"text\"] = \" \".join(data[2])\n",
    "    pid_to_passage_to_save.append(data_temp)\n",
    "print(f\"courpus_len == {len(pid_to_passage)}\")\n",
    "pid_to_passage_to_save[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building TDF matrix: 100%|██████████| 1000000/1000000 [00:30<00:00, 32724.59it/s]\n",
      "Building inverted index: 100%|██████████| 586321/586321 [03:11<00:00, 3062.42it/s]\n"
     ]
    }
   ],
   "source": [
    "## 构建搜索引擎\n",
    "serachengine_obj = SearchEngine(model=\"bm25\").index(pid_to_passage_to_save)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载 训练集数据 \"train.queries.tsv\" 并进行搜索 保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.queries.tsv\n",
      "分词完成\n",
      "train_qid_len == 96279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '6', 'text': '谍战 电视剧 战争'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_qid_to_query = []\n",
    "datapath = \"train.queries.tsv\"\n",
    "\n",
    "\n",
    "def create_id_item_dict(file_path: str, delimiter: str = \"\\t\") -> Dict[int, str]:  ## note:数据准\n",
    "\n",
    "    print(file_path)\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "\n",
    "        for idx, (qid, passage) in enumerate(csv.reader(f, delimiter=delimiter)):  ## ['qid', 'pid', 'index']\n",
    "\n",
    "            train_qid_to_query.append((qid, passage, [word for word in jieba.cut(passage) if word not in stopwords]))\n",
    "\n",
    "\n",
    "create_id_item_dict(datapath)\n",
    "print(\"分词完成\")\n",
    "train_qid_to_query_to_save = []\n",
    "for data in train_qid_to_query:\n",
    "    data_temp = {}\n",
    "    data_temp[\"id\"] = data[0]\n",
    "    data_temp[\"text\"] = \" \".join(data[2])\n",
    "    train_qid_to_query_to_save.append(data_temp)\n",
    "print(f\"train_qid_len == {len(train_qid_to_query)}\")\n",
    "train_qid_to_query_to_save[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch search: 100%|██████████| 96279/96279 [02:37<00:00, 612.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_bm25_len == 96279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## 搜索结果\n",
    "train_bm25_top100_res = serachengine_obj.bsearch(train_qid_to_query_to_save,cutoff=100,batch_size= 10000)\n",
    "print(f\"train_bm25_len == {len(train_bm25_top100_res)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to train.top100.bm25.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96279/96279 [00:31<00:00, 3023.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 8958455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_path = \"train.top100.bm25.tsv\"\n",
    "print(f\"save to {save_path}\")\n",
    "\n",
    "fp = open(save_path, \"w\", newline=\"\")\n",
    "writer = csv.writer(fp, delimiter=\"\\t\")\n",
    "count = 0\n",
    "writer.writerow([\"qid\", \"pid\", \"rank\", \"score\"])\n",
    "for qid, rels_ in tqdm(train_bm25_top100_res.items()):\n",
    "    rank = 1\n",
    "    for pid, score in rels_.items():\n",
    "        writer.writerow([qid, pid, rank, score])\n",
    "        rank += 1\n",
    "        count += 1\n",
    "fp.close()\n",
    "print(f\"count = {count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载验证数据 dev_queries.tsv  并进行搜索 保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.queries.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 199188.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_qid_to_query = []\n",
    "datapath =\"dev.queries.tsv\"\n",
    "def create_id_item_dict(file_path: str, delimiter: str = \"\\t\") -> Dict[int, str]: ## note:数据准\n",
    "    \n",
    "    print(file_path)\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for idx,(qid,passage) in  enumerate(csv.reader(f, delimiter=delimiter)):  ## ['qid', 'pid', 'index']\n",
    "            dev_qid_to_query.append((qid,passage,[word for word in jieba.cut(passage) if word not in stopwords]))\n",
    "\n",
    "create_id_item_dict(datapath,)\n",
    "dev_pid_to_query_to_save=[]\n",
    "for data in tqdm(dev_qid_to_query):\n",
    "    data_temp = {}\n",
    "    data_temp[\"id\"]=data[0]\n",
    "    data_temp[\"text\"]= \" \".join(data[2])\n",
    "    dev_pid_to_query_to_save.append(data_temp)\n",
    "dev_pid_to_query_to_save[:10]\n",
    "print(len(dev_pid_to_query_to_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch search: 100%|██████████| 1000/1000 [00:01<00:00, 698.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_bm25_len == 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_bm25_top100_res = serachengine_obj.bsearch(dev_pid_to_query_to_save,cutoff=100,batch_size= 10000)\n",
    "\n",
    "print(f\"dev_bm25_len == {len(dev_bm25_top100_res)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to dev.top100.bm25.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3161.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_len=92342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_path = \"dev.top100.bm25.tsv\"\n",
    "print(f\"save to {save_path}\")\n",
    "\n",
    "fp = open(save_path, \"w\", newline=\"\")\n",
    "writer = csv.writer(fp, delimiter=\"\\t\")\n",
    "count = 0\n",
    "writer.writerow([\"qid\", \"pid\", \"rank\",\"score\"])\n",
    "for qid,v in tqdm(dev_bm25_top100_res.items()):\n",
    "    rank = 1\n",
    "    for pid,score in v.items():\n",
    "        writer.writerow([qid, pid, rank,score])\n",
    "        rank += 1\n",
    "        count+=1\n",
    "fp.close()\n",
    "print(f\"count_len={count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test atutotune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_obj = {}\n",
    "datapath = \"dev.qrels.tsv\"\n",
    "\n",
    "def get_qrels(file_path: str, delimiter: str = \"\\t\") -> Dict[int, str]:  ## note:数据准\n",
    "\n",
    "    print(file_path)\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "\n",
    "        for idx, (qid ,_, pid,rel) in enumerate(tqdm(csv.reader(f, delimiter=delimiter))):  ## ['qid', 'pid', 'index']\n",
    "               if  qid in qrels_obj.keys():\n",
    "                    qrels_obj[qid][pid]=rel\n",
    "               else:\n",
    "                    qrels_obj[qid]={}\n",
    "                    qrels_obj[qid][pid]=rel\n",
    "\n",
    "\n",
    "get_qrels(datapath)\n",
    "qrels_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1079893af94f4c8808664d2c83248f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "serachengine_obj.autotune(\n",
    "  queries=dev_pid_to_query_to_save,  # Train queries\n",
    "  qrels=qrels_obj,      # Train qrels\n",
    "  metric=\"ndcg\",  # Default value, metric to maximize\n",
    "  n_trials=100,   # Default value, number of trials\n",
    "  cutoff=100,     # Default value, number of results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 0.85, 'k1': 0.2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serachengine_obj.hyperparams\n",
    "## BM25 公式中包含 3 个自由调节参数 ，除了调节因子 b 外 ，还有针对词频的调节因子 k1和 k2。 k1的作用是对查询词在文档中的词频进行调节，如果将 k1设定为 0，则第二部分计算因子成了整数 1，即不考虑词频的因素，退化成了二元独立模型。 如果将 k1设定为较大值， 则第二部分计算因子基本和词频 fi保持线性增长，即放大了词频的权值，根据经验，一般将 k1设定为 1.2。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yyb_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
